{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import statsmodels as sm\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compute the portfolio's daily returns\n",
    "```python\n",
    "asset_returns = asset_prices.pct_change()\n",
    "portfolio_returns = asset_returns.dot(weights)\n",
    "```\n",
    "- Plot portfolio returns\n",
    "```python\n",
    "portfolio_returns.plot().set_ylabel(\"Daily Return, %\")\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the portfolio's daily returns\n",
    "asset_returns = asset_prices.pct_change()\n",
    "portfolio_returns = asset_returns.dot(weights)\n",
    "\n",
    "# Plot portfolio returns\n",
    "portfolio_returns.plot().set_ylabel(\"Daily Return, %\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the covariance matrix from portfolio asset's returns\n",
    "covariance = asset_returns.cov()\n",
    "\n",
    "# Annualize the covariance using 252 trading days per year\n",
    "covariance = covariance * 252\n",
    "\n",
    "# Display the covariance matrix\n",
    "print(covariance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and display portfolio volatility for 2008 - 2009\n",
    "portfolio_variance = np.transpose(weights) @ covariance @ weights\n",
    "portfolio_volatility = np.sqrt(portfolio_variance)\n",
    "print(portfolio_volatility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the 30-day rolling window of portfolio returns\n",
    "returns_windowed = portfolio_returns.rolling(30)\n",
    "\n",
    "# Compute the annualized volatility series\n",
    "volatility_series = returns_windowed.std() *np.sqrt(252)\n",
    "\n",
    "# Plot the portfolio volatility\n",
    "volatility_series.plot().set_ylabel(\"Annualized Volatility, 30-day Window\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Risk Factors: variables or events driving portfolio return & volatility\n",
    "* Volatility: a measure of dispersion of returns around expected values\n",
    "* Risk exposure: measure of possible portfolio loss\n",
    "## Risk factors: determine risk exposure\n",
    "* Systematic Risk: Unhedgeable risks that affect all participants\n",
    "* Idiosyncratic risk: hedgeable risk that can be hedged or mitigated by transferrence\n",
    "## Factor models: assessment of risk factors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the investment portfolio price data into the price variable.\n",
    "prices = pd.read_csv(\"portfolio.csv\")\n",
    "\n",
    "# Convert the 'Date' column to a datetime index\n",
    "prices['Date'] = pd.to_datetime(prices['Date'], format='%d/%m/%Y')\n",
    "prices.set_index(['Date'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the mean_historical_return method\n",
    "from pypfopt.expected_returns import mean_historical_return\n",
    "\n",
    "# Compute the annualized average historical return\n",
    "mean_returns = mean_historical_return(prices, frequency = 252)\n",
    "\n",
    "# Plot the annualized average historical return\n",
    "plt.plot(mean_returns, linestyle = 'None', marker = 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the CovarianceShrinkage object\n",
    "from pypfopt.risk_models import CovarianceShrinkage\n",
    "\n",
    "# Create the CovarianceShrinkage instance variable\n",
    "cs = CovarianceShrinkage(prices)\n",
    "\n",
    "# Compute the sample covariance matrix of returns\n",
    "sample_cov = prices.pct_change().cov() * 252\n",
    "\n",
    "# Compute the efficient covariance matrix of returns\n",
    "e_cov = cs.ledoit_wolf()\n",
    "\n",
    "# Display both the sample covariance_matrix and the efficient e_cov estimate\n",
    "print(\"Sample Covariance Matrix\\n\", sample_cov, \"\\n\")\n",
    "print(\"Efficient Covariance Matrix\\n\", e_cov, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of time periods (or 'epochs')\n",
    "epochs = { 'before' : {'start': '1-1-2005', 'end': '31-12-2006'},\n",
    "           'during' : {'start': '1-1-2007', 'end': '31-12-2008'},\n",
    "           'after'  : {'start': '1-1-2009', 'end': '31-12-2010'}\n",
    "         }\n",
    "\n",
    "# Compute the efficient covariance for each epoch\n",
    "e_cov = {}\n",
    "for x in epochs.keys():\n",
    "  sub_price = prices.loc[epochs[x]['start']:epochs[x]['end']]\n",
    "  e_cov[x] = CovarianceShrinkage(sub_price).ledoit_wolf()\n",
    "\n",
    "# Display the efficient covariance matrices for all epochs\n",
    "print(\"Efficient Covariance Matrices\\n\", e_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Crtical Line Algorithm object\n",
    "efficient_portfolio_during = CLA(returns_during, ecov_during)\n",
    "\n",
    "# Find the minimum volatility portfolio weights and display them\n",
    "print(efficient_portfolio_during.min_volatility())\n",
    "\n",
    "# Compute the efficient frontier\n",
    "(ret, vol, weights) = efficient_portfolio_during.efficient_frontier()\n",
    "\n",
    "# Add the frontier to the plot showing the 'before' and 'after' frontiers\n",
    "plt.scatter(vol, ret, s = 4, c = 'g', marker = '.', label = 'During')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring Risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the VaR measure at the 95% confidence level using norm.ppf()\n",
    "VaR_95 = norm.ppf(0.95)\n",
    "\n",
    "# Create the VaR meaasure at the 5% significance level using numpy.quantile()\n",
    "draws = norm.rvs(size = 100000)\n",
    "VaR_99 = np.quantile(draws, 0.99)\n",
    "\n",
    "# Compare the 95% and 99% VaR\n",
    "print(\"95% VaR: \", VaR_95, \"; 99% VaR: \", VaR_99)\n",
    "\n",
    "# Plot the normal distribution histogram and 95% VaR measure\n",
    "plt.hist(draws, bins = 100)\n",
    "plt.axvline(x = VaR_95, c='r', label = \"VaR at 95% Confidence Level\")\n",
    "plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Risk exposure and loss\n",
    "\n",
    "# Import the Student's t-distribution\n",
    "from scipy.stats import t\n",
    "\n",
    "# Create rolling window parameter list\n",
    "mu = losses.rolling(30).mean()\n",
    "sigma = losses.rolling(30).std()\n",
    "rolling_parameters = [(29, mu[i], s) for i,s in enumerate(sigma)]\n",
    "\n",
    "# Compute the 99% VaR array using the rolling window parameters\n",
    "VaR_99 = np.array( [ t.ppf(0.99, *params) \n",
    "                    for params in rolling_parameters ] )\n",
    "\n",
    "# Plot the minimum risk exposure over the 2005-2010 time period\n",
    "plt.plot(losses.index, 0.01 * VaR_99 * 100000)\n",
    "plt.show()\n",
    "\n",
    "# Fit the Student's t distribution to crisis losses\n",
    "p = t.fit(crisis_losses)\n",
    "\n",
    "# Compute the VaR_99 for the fitted distribution\n",
    "VaR_99 = t.ppf(0.99, *p)\n",
    "\n",
    "# Use the fitted parameters and VaR_99 to compute CVaR_99\n",
    "tail_loss = t.expect( lambda y: y, args = (p[0],), loc = p[1], scale = p[2], lb = VaR_99 )\n",
    "CVaR_99 = (1 / (1 - 0.99)) * tail_loss\n",
    "print(CVaR_99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Risk management using VaR & CVaR\n",
    "\n",
    "# Import the EfficientFrontier class\n",
    "from pypfopt.efficient_frontier import EfficientFrontier\n",
    "\n",
    "# Import the negative_cvar objective function\n",
    "from pypfopt.objective_functions import negative_cvar\n",
    "\n",
    "# Create the efficient frontier instance\n",
    "ef = EfficientFrontier(None, e_cov)\n",
    "\n",
    "# Find the cVar-minimizing portfolio weights at the default 95% confidence level\n",
    "optimal_weights = ef.custom_objective(negative_cvar, returns)\n",
    "\n",
    "# Display the optimal weights\n",
    "print(optimal_weights)\n",
    "\n",
    "# Initialize the efficient portfolio dictionary\n",
    "ef_dict = {}\n",
    "\n",
    "# For each epoch, assign an efficient frontier instance to ef\n",
    "for x in ['before', 'during', 'after']: \n",
    "    ef_dict[x] = EfficientFrontier(None, e_cov_dict[x])\n",
    "    \n",
    "    # Initialize the dictionary of optimal weights\n",
    "optimal_weights_dict = {}\n",
    "\n",
    "# Find and display the CVaR-minimizing portfolio weights at the default 95% confidence level\n",
    "for x in ['before', 'during', 'after']:\n",
    "    optimal_weights_dict[x] = ef_dict[x].custom_objective(negative_cvar, returns_dict[x])\n",
    "    \n",
    "# Compare the CVaR-minimizing weights to the minimum volatility weights for the 'before' epoch\n",
    "print(\"CVaR:\\n\", pd.DataFrame.from_dict(optimal_weights_dict['before']), \"\\n\")\n",
    "print(\"Min Vol:\\n\", pd.DataFrame.from_dict(min_vol_dict['before']), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Portfolio Hedging: offsetting risk\n",
    "## Hedge instruments: options\n",
    "* **Derivative:** hedge instrument\n",
    "* **European option:** very popular derivative\n",
    "    * European **Call** Option: right(not obligation) to **purchase** asset at fixed price (*X*) on date (*M*)\n",
    "    * European **Put** Option: right(not obligation) to **sell** asset at fixed price (*X*) on date (*M*)\n",
    "    * Stock = \"underlying\" of the option\n",
    "    * Current market price(*S*) = spot price\n",
    "    * strike price(*X*)\n",
    "    * maturity date(*M*) \n",
    "## Black-Scholes option pricing\n",
    "* Option value changes when price of underlying changes => can be used to **hedge risk**\n",
    "* Need to **value** option: requires assumptions about *market, underlying, interest rate, etc.*\n",
    "* **Black_Scholes** option pricing formula requires the following variables for time *t*:\n",
    "    * spot price(*S*)\n",
    "    * strike price(*X*)\n",
    "    * time to maturity *T*: *M - t*\n",
    "    * risk-free interest rate *r*\n",
    "    * volatility of underlying returns: standard deviation\n",
    "## Black-Scholes formula assumptions\n",
    "* Market structure\n",
    "    * Efficient markets\n",
    "    * No transaction costs\n",
    "    * Risk-free interest rate\n",
    "* Underlying stock\n",
    "    * No dividends\n",
    "    * Normally distributed returns \n",
    "## Computing the Black-Scholes option value\n",
    "* Black-Scholes option pricing formula `black_scholes()`\n",
    "* Required parameters: *S,X,T*(in fractions of year), *r*, standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the volatility as the annualized standard deviation of IBM returns\n",
    "sigma = np.sqrt(252) * IBM_returns.std()\n",
    "\n",
    "# Compute the Black-Scholes option price for this volatility\n",
    "value_s = black_scholes(S = 90, X = 80, T = 0.5, r = 0.02, \n",
    "                        sigma = sigma, option_type = \"call\")\n",
    "\n",
    "# Compute the Black-Scholes option price for twice the volatility\n",
    "value_2s = black_scholes(S = 90, X = 80, T = 0.5, r = 0.02, \n",
    "                sigma = sigma * 2, option_type = \"call\")\n",
    "\n",
    "# Display and compare both values\n",
    "print(\"Option value for sigma: \", value_s, \"\\n\",\n",
    "      \"Option value for 2 * sigma: \", value_2s)\n",
    "\n",
    "# Select the first 100 observations of IBM data\n",
    "IBM_spot = IBM[:100]\n",
    "\n",
    "# Initialize the European put option values array\n",
    "option_values = np.zeros(IBM_spot.size)\n",
    "\n",
    "# Iterate through IBM's spot price and compute the option values\n",
    "for i,S in enumerate(IBM_spot.values):\n",
    "    option_values[i] = black_scholes(S = S, X = 140, T = 0.5, r = 0.02, \n",
    "                        sigma = sigma, option_type = \"put\")\n",
    "\n",
    "# Display the option values array\n",
    "option_axis.plot(option_values, color = \"red\", label = \"Put Option\")\n",
    "option_axis.legend(loc = \"upper left\")\n",
    "plt.show()\n",
    "\n",
    "# Compute the annualized standard deviation of `IBM` returns\n",
    "sigma = np.sqrt(252) * IBM_returns.std()\n",
    "\n",
    "# Compute the Black-Scholes value at IBM spot price 70\n",
    "value = black_scholes(S = 70, X = 80, T = 0.5, r = 0.02, \n",
    "                      sigma = sigma, option_type = \"put\")\n",
    "# Find the delta of the option at IBM spot price 70\n",
    "delta = bs_delta(S = 70, X = 80, T = 0.5, r = 0.02, \n",
    "                 sigma = sigma, option_type = \"put\")\n",
    "\n",
    "# Find the option value change when the price of IBM falls to 69.5\n",
    "value_change = black_scholes(S = 69.5, X = 80, T = 0.5, r = 0.02, \n",
    "                             sigma = sigma, option_type = \"put\") - value\n",
    "\n",
    "print( (69.5 - 70) + (1/delta) * value_change )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parametric Estimation\n",
    "## A class of distributions\n",
    "* **Loss distribution:** not known with certainty\n",
    "* *Class* of possible distributions?\n",
    "    * Suppose class of distributions *f*(*x*, $\\Theta$)\n",
    "    * $\\Theta$ is a vector of unknown **parameters**\n",
    "* **Example:** Normal distribution\n",
    "    * Parameters: $\\Theta$ = ($\\mu$, $\\sigma$), mean and standard deviation\n",
    "* **Parametric estimation:** find the \"best\" theta given data\n",
    "## Fitting a distribution\n",
    "* Fit distribution according to error-minimizing criteria\n",
    "    * **Example:** `scipy.stats.norm.fit()`, fitting Normal distribution to data\n",
    "        * **Result:** optimally fitted mean and standard deviation\n",
    "* **Advantages:**\n",
    "    * Can *visualize* difference between data and estimate using histogram\n",
    "    * Can provide *goodness-of-fit* test\n",
    "## Anderson-Darling test\n",
    "* Statistical test of goodness of fit\n",
    "    * Test null hypothesis: data are Normally distributed\n",
    "    * Test statistic rejects Normal distribution if larger than `critical values`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import anderson\n",
    "anderson(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skewness\n",
    "* Skewness: degree to which data is non-symmetrically distributed\n",
    "## Testing for skewness\n",
    "* Test how far data is from symmetric distribution:`scipy.stats.skewtest`\n",
    "* *Null hypothesis*: no skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skewtest\n",
    "skewtest(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Normal distribution and skewness test from scipy.stats\n",
    "from scipy.stats import norm, anderson\n",
    "\n",
    "# Fit portfolio losses to the Normal distribution\n",
    "params = norm.fit(losses)\n",
    "\n",
    "# Compute the 95% VaR from the fitted distribution, using parameter estimates\n",
    "VaR_95 = norm.ppf(0.95, *params)\n",
    "print(\"VaR_95, Normal distribution: \", VaR_95)\n",
    "\n",
    "# Test the data for Normality\n",
    "print(\"Anderson-Darling test result: \", anderson(losses))\n",
    "\n",
    "# Import the skew-normal distribution and skewness test from scipy.stats\n",
    "from scipy.stats import skewnorm, skewtest\n",
    "\n",
    "# Test the data for skewness\n",
    "print(\"Skewtest result: \", skewtest(losses))\n",
    "\n",
    "# Fit the portfolio loss data to the skew-normal distribution\n",
    "params = skewnorm.fit(losses)\n",
    "\n",
    "# Compute the 95% VaR from the fitted distribution, using parameter estimates\n",
    "VaR_95 = skewnorm.ppf(0.95, *params)\n",
    "print(\"VaR_95 from skew-normal: \", VaR_95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Historical and Monte Carlo simulation\n",
    "## Historical simulation\n",
    "* No appropriate class of distributions?\n",
    "* **Historical simulation:** use past to predict future\n",
    "    * No distributional assumption required\n",
    "    * Data about previous losses become *simulated* losses for tomorrow \n",
    "## Monte Carlo simulation\n",
    "* **Monte Carlo simulation:** powerful combination of parametric estimation and simulation\n",
    "    * Assumes distribution(s) for portfolio loss and/or risk factors\n",
    "    * Relies upon random draws from distribution(s) to create random *path*, called a *run*\n",
    "    * Repeat random draws => creates **set** of simulation runs\n",
    "* Compute simulated portfolio loss over *each* run, up to a desired time\n",
    "* Find VaR estimate as quantile of simulated losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Historical simulation in Python\n",
    "* **VaR:** start with returns in `asset_returns`\n",
    "* Compute `portfolio_returns` using `portfolio_weights`\n",
    "* Covert `portfolio_returns into `losses`\n",
    "* VaR: compute `np.quantile()` for `losses` at desired confidence level\n",
    "* Assumes future distribution of losses is *exactly* the same as the past"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming an equal weight portfolio of four stocks\n",
    "portfolio_weights = [0.25,0.25,0.25,0.25]\n",
    "portfolio_returns = asset_returns.dot(weights)\n",
    "losses = -portfolio_returns\n",
    "VaR_95 = np.quantile(losses, 0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo simulation in Python (Assuming Normal distribution of portfolio losses)\n",
    "* **Step one:**\n",
    "    * Import Normal distribution `norm` from `scipy.stats`\n",
    "    * Define `total_steps` (1day = 1440 minutes)\n",
    "    * Define the number of runs `N`\n",
    "    * Compute mean `mu` and standard deviation `sigma` of `portfolio_losses` data\n",
    "* **Step two:**\n",
    "    * Initialize `daily_loss` vector for `N` runs\n",
    "    * Loop over `N` runs\n",
    "        * Compute Monte Carlo simulated `loss` vector\n",
    "        * Uses `norm.rvs()` to draw repeatedly from standard Normal distribution\n",
    "        * Draws match data using `mu` and `sigma` scaled by 1/`total_steps`\n",
    "* **Step three:**\n",
    "    * Generate cumulative `daily_loss`, for each run `n`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "total_steps = 1440\n",
    "N = 10000\n",
    "mu = portfolio_losses.mean()\n",
    "sigma = portfolio_losses.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_losses = np.zeros(N)\n",
    "for n in range(N):\n",
    "    loss = (mu * (1/total_steps) + \n",
    "           norm.rvs(size = total_steps) * sigma * np.sqrt(1/total_steps))\n",
    "    \n",
    "# Example\n",
    "# Do not run\n",
    "\n",
    "# Initialize daily cumulative loss for the assets, across N runs\n",
    "daily_loss = np.zeros((4,N))\n",
    "\n",
    "# Create the Monte Carlo simulations for N runs\n",
    "for n in range(N):\n",
    "    # Compute simulated path of length total_steps for correlated returns \n",
    "    correlated_randomness = e_cov @ norm.rvs(size = (4,total_steps))\n",
    "    # Adjust simulated path by total_steps and mean of portfolio losses\n",
    "    steps = 1/total_steps\n",
    "    minute_losses = mu * steps + correlated_randomness * np.sqrt(steps)\n",
    "    daily_loss[:, n] = minute_losses.sum(axis=1)\n",
    "    \n",
    "# Generate the 95% VaR estimate\n",
    "losses = weights @ daily_loss\n",
    "print(\"Monte Carlo VaR_95 estimate: \", np.quantile(losses, 0.95))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structural breaks\n",
    "## Chow test\n",
    "* **Chow test:** identifies statistical significance of a possible structural break\n",
    "    * **Requires:** *pre-specified* point of structural break\n",
    "    * **Requires:** *linear* relation (e.g. factor model) \n",
    "    \\begin{equation}        \n",
    "    log(Population_{t}) = \\alpha + \\beta * Year_{t} + \\upsilon_{t}$$\n",
    "    \\end{equation}\n",
    "* Structural break indications\n",
    "    * Visualization of trend may not indicate break point\n",
    "    * Alternative: examine **volatility** rather than trend\n",
    "        * Structural change often accompanied by greater uncertainty(volatility)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mort_del' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-8967292662ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Add a constant to the regression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mmort_del\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_constant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmort_del\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Regress quarterly minimum portfolio returns against mortgage delinquencies\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mort_del' is not defined"
     ]
    }
   ],
   "source": [
    "# Import the statsmodels API to be able to run regressions\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Add a constant to the regression\n",
    "mort_del = sm.add_constant(mort_del)\n",
    "\n",
    "# Regress quarterly minimum portfolio returns against mortgage delinquencies\n",
    "result = sm.OLS(port_q_min, mort_del).fit()\n",
    "\n",
    "# Retrieve the sum-of-squared residuals\n",
    "ssr_total = result.ssr\n",
    "print(\"Sum-of-squared residuals, 2005-2010: \", ssr_total)\n",
    "\n",
    "# Add intercept constants to each sub-period 'before' and 'after'\n",
    "before_with_intercept = sm.add_constant(before['mort_del'])\n",
    "after_with_intercept  = sm.add_constant(after['mort_del'])\n",
    "\n",
    "# Fit OLS regressions to each sub-period\n",
    "r_b = sm.OLS(before['returns'], before_with_intercept).fit()\n",
    "r_a = sm.OLS(after['returns'],  after_with_intercept).fit()\n",
    "\n",
    "# Get sum-of-squared residuals for both regressions\n",
    "ssr_before = r_b.ssr\n",
    "ssr_after = r_a.ssr\n",
    "# Compute and display the Chow test statistic\n",
    "numerator = ((ssr_total - (ssr_before + ssr_after)) / 2)\n",
    "denominator = ((ssr_before + ssr_after) / (24 - 4))\n",
    "print(\"Chow test statistic: \", numerator / denominator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Volatility and extreme values\n",
    "### Rolling window volatility\n",
    "* **Rolling window:** compute volatility over time and detect changes\n",
    "* To create a 30 day rolling window `portfolio_returns.rolling(30)`\n",
    "* Compute the volatility of the rolling window (drop unavailable dates)\n",
    "* Compute summary statistics of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling = portfolio_returns.rolling(30)\n",
    "volatility = rolling.std().dropna()\n",
    "vol_mean = volatility.resample(\"M\").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Visualize resulting volatility ($\\sigma^2$ or $\\sigma$)\n",
    "* Large *changes in volatility* indicates possible structural break point(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_mean.pct_change().plot(title = \"$\\Delta$ average volatility\").set_ylabel(\"% $\\Delta$ stdev\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extreme value theory\n",
    "* statistical distribution of extreme values\n",
    "* **Block maxima:** \n",
    "    * Break period into sub_periods\n",
    "    * Form *blocks* from each sub-period\n",
    "    * Set of block maxima = dataset\n",
    "* **Peak over threshold** (POT):\n",
    "\n",
    "* **Example:** Block maxima for 2007-2009 \n",
    "    * Resample losses with desired period(e.g. weekly)\n",
    "    ```python\n",
    "    maxima = losses.resample(\"W\").max()\n",
    "    ```\n",
    "* **Generalized Extreme Value Distribution (GEV)**\n",
    "    * Distribution of *maxima* of data\n",
    "    * Example: parametric estimation using `scipy.stats.genextreme`\n",
    "    ```python\n",
    "    from scipy.stats import genextreme\n",
    "    params = genextreme.fit(maxima)\n",
    "    ```\n",
    "### VaR and CVaR from GEV distribution\n",
    "* **99% VaR** from GEV distribution\n",
    "    * Use `.ppf()` percent point function to find 99% VaR\n",
    "    * Requires `params` from fitted GEV distribution\n",
    "    ```python\n",
    "    VaR_99 = genextreme.ppf(0.99, *params)\n",
    "    ```\n",
    "* **99% CVaR** from GEV distribution\n",
    "    * CVaR is conditional expectation of loss given VaR as minimum loss\n",
    "    * Use `expect()` method to find expected value\n",
    "    ```python\n",
    "    CVaR_99 = (1 / (1-0.99) ) * genextreme.expect(lambda x: x, *params, lb = VaR_99)\n",
    "    ```   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covering losses\n",
    "* **Risk management:** covering losses\n",
    "    * Regulatory requirement in many industries(banking, insurance)\n",
    "    * Reserves must be available to cover losses over a *specified period* at a *specified confidence level*\n",
    "* **VaR from GEV distribution:**\n",
    "    * estimates maximum loss\n",
    "        * givien period\n",
    "        * given confidence interval\n",
    "        \n",
    "* **Example:** Initial portfolio value = 1,000,000 USD\n",
    "* **One week reserve requirement** at 99% confidence\n",
    "    * $VaR_{99}$ from GEV distribution: maximum loss over one wee at 99% confidence\n",
    "* **Reserve requirement:** Portfolio value * $VaR_{99}$\n",
    "    * Suppose $VaR_{99}$ = 0.10, i.e. 10% maximum loss\n",
    "    * Reserve requirement = 100,000 USD\n",
    "* Portfolio value changes result on reserve requirement changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don not run \n",
    "# Example\n",
    "\n",
    "# Compute the weekly block maxima for GE's stock\n",
    "weekly_maxima = losses.resample(\"W\").max()\n",
    "\n",
    "# Fit the GEV distribution to the maxima\n",
    "p = genextreme.fit(weekly_maxima)\n",
    "\n",
    "# Compute the 99% VaR (needed for the CVaR computation)\n",
    "VaR_99 = genextreme.ppf(0.99, *p)\n",
    "\n",
    "# Compute the 99% CVaR estimate\n",
    "CVaR_99 = (1 / (1 - 0.99)) * genextreme.expect(lambda x: x, \n",
    "           args=(p[0],), loc = p[1], scale = p[2], lb = VaR_99)\n",
    "\n",
    "# Display the covering loss amount\n",
    "print(\"Reserve amount: \", 1000000 * CVaR_99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Density Estimation\n",
    "* Up to this point Risk factor distributions have been:\n",
    "    * *Assumed*(e.g.Normal,Student-T,etc.)\n",
    "    * *Fitted*(parametric estimation, Monte Carlo simulation)\n",
    "    * *Ignored*(historical simulation)\n",
    "* **Actual data:** represented by a histogram\n",
    "* How to represent histogram by probability distribution?\n",
    "    * *Smooth* data by **filtering**\n",
    "    * **Non-parametric estimation**\n",
    "* **Data smoothing**\n",
    "    * **Filter:** smoothen out 'bumps' of histogram\n",
    "\n",
    "### Kernel Density Estimation (KDE) in Python\n",
    "```python\n",
    "from scipy.stats import gaussian_kde\n",
    "kde = gaussian_kde(losses)\n",
    "loss_range = np.linspace(np.min(losses),\n",
    "                         np.max(losses),\n",
    "                         1000)\n",
    "plt.plot(loss_range, kde.pdf(loss_range))\n",
    "```   \n",
    "* Visualization: probability density function from KDE fit\n",
    "\n",
    "### Finding VaR using KDE\n",
    "* VaR: use `gaussian_kde` `.resample()` method\n",
    "* Find quantile of resulting sample\n",
    "```python\n",
    "sample = kde.resample(size = 1000)\n",
    "VaR_99 = np.quantile(sample, 0.99)\n",
    "print(\"VaR_99 from KDE: \", VaR_99)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don not run\n",
    "# Example\n",
    "\n",
    "# Generate a fitted T distribution over losses\n",
    "params = t.fit(losses)\n",
    "\n",
    "# Generate a Gaussian kernal density estimate over losses\n",
    "kde = gaussian_kde(losses)\n",
    "\n",
    "# Add the PDFs of both estimates to a histogram, and display\n",
    "loss_range = np.linspace(np.min(losses), np.max(losses), 1000)\n",
    "axis.plot(loss_range, t.pdf(loss_range, *params), label = 'T distribution')\n",
    "axis.plot(loss_range, kde.pdf(loss_range), label = 'Gaussian KDE')\n",
    "plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network risk management\n",
    "### Real-time portfolio updating\n",
    "* **Risk management**\n",
    "    * **Defined** risk measures(VaR,CVaR)\n",
    "    * **Estimated** risk measures(parametric, historical, Monteo Carlo)\n",
    "    * **Optimized** portfolio(Modern Portfolio Theory(MPT))\n",
    "* *New* market information requires *updated* portfolio weights\n",
    "    * **Problem:** portfolio optimization is costly\n",
    "    * **Solution:** weights = *f*(prices)\n",
    "    * To evaluate *f* in real-time\n",
    "    * Update *f* only occasionally\n",
    "* **Neural network:** output = *f*(input)\n",
    "    * **Neuron:** interconnected processing node in function\n",
    "    * **Neural network structure:**\n",
    "        * Input layer\n",
    "        * Hidden layer\n",
    "        * Output layer\n",
    "    * **Training:** learn the relationship between input and output    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating neural networks in Python\n",
    "* **Keras:** high-level Python library for neural networks/deep learning\n",
    "```python\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=4, activation='sigmoid'))\n",
    "model.add(Dense(4))\n",
    "```\n",
    "* **Training the network in Python**\n",
    "* Historical asset prices:`training_input` matrix\n",
    "* Historical portfolio weights:`training_output`vector\n",
    "* **Compile** model with:\n",
    "    * given error minimization(**\"loss\"**)\n",
    "    * given optimization algorithm (**\"optimizer\"**)\n",
    "    * **Fit** model to training data\n",
    "        * **epochs:** number of training loops to update internal parameters\n",
    "```python\n",
    "model.compile(loss='mean_squared_error',optimizer='rmsprop')\n",
    "model.fit(training_input, training_output, epochs=100)\n",
    "```        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risk management in Python\n",
    "* **Usage:** provide new asset pricing data\n",
    "    * **New vector** `new_asset_prices` given to input layer\n",
    "* Evaluate network using `model.predict()` on new prices\n",
    "    * **Result:** `predicted` portfolio weights\n",
    "```python\n",
    "predicted = model.predict(new_asset_prices)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not run\n",
    "# Example \n",
    "\n",
    "# Create the training values from the square root function\n",
    "y = np.sqrt(x)\n",
    "\n",
    "# Create the neural network\n",
    "model = Sequential()\n",
    "model.add(Dense(16, input_dim=1, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Train the network\n",
    "model.compile(loss='mean_squared_error', optimizer='rmsprop')\n",
    "model.fit(x, y, epochs=100)\n",
    "\n",
    "## Plot the resulting approximation and the training values\n",
    "plt.plot(x, y, x, model.predict(x))\n",
    "plt.show()\n",
    "\n",
    "# Set the input and output data\n",
    "training_input = prices.drop('Morgan Stanley', axis=1)\n",
    "training_output = prices['Morgan Stanley']\n",
    "\n",
    "# Create and train the neural network with two hidden layers\n",
    "model = Sequential()\n",
    "model.add(Dense(16, input_dim=3, activation='sigmoid'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mean_squared_logarithmic_error', optimizer='rmsprop')\n",
    "model.fit(training_input, training_output, epochs=100)\n",
    "\n",
    "# Scatter plot of the resulting model prediction\n",
    "axis.scatter(training_output, model.predict(training_input)); plt.show()\n",
    "\n",
    "# Create neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim = 4, activation = 'relu'))\n",
    "model.add(Dense(64, activation = 'relu'))\n",
    "model.add(Dense(4, activation = 'relu'))\n",
    "\n",
    "# Use the pre-trained model to predict portfolio weights given new asset returns\n",
    "asset_returns = np.array([0.001060, 0.003832, 0.000726, -0.002787])\n",
    "asset_returns.shape = (1,4)\n",
    "print(\"Predicted minimum volatility portfolio: \", pre_trained_model.predict(asset_returns))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
